
MASTER OF DATA SCIENCE (SEMESTER 2 - 2022/2023)
FACULTY OF COMPUTER SCIENCE AND INFORMATION TECHNOLOGY 
WQD7005 DATA MINING GROUP 3

GROUP ASSIGNMENT
Project Title: Productivity Prediction of Garment Employees
Instructor: Prof Dr Teh Ying Wah

Group 2 S2172813JIA WEI ENGS2175592SHI SUN LAIS2176564WEN HUI SEES2141959TANG JINGFA1. Introduction
　　　As global garment demand increases, enhancing productivity is vital for manufacturers to maintain growth, cut costs, and stay competitive. In the labour-intensive apparel industry, employee productivity is crucial for overall business performance. Understanding factors affecting productivity and developing accurate predictive models is essential .
　　　Technological advancements have revolutionised the apparel industry, boosting productivity. However, automation cannot completely replace human employees, making individual productivity evaluation a challenge. Data-driven insights into productivity determinants can help companies make informed decisions to motivate their workforce and secure a competitive advantage.
　　　This project aims to illustrate how data mining and predictive analytics can be employed to address critical real-world challenges. Tackling productivity issues in garment manufacturing by adopting a data-driven approach to understanding key determinants can enhance the competitiveness of businesses and contribute to the expansion of the garment industry.
　　　
2. Our Dataset
2.1. Dataset Description
　　　The dataset used is "Productivity Prediction of Garment Employees Data Set" and has been available on the UCI Machine Learning Repository since 2020.
　　　There are a total of 1197 instances and 15 attributes. It contains attributes related to garment manufacturing and employee productivity such as date, quarter, department, day, team, targeted_productivity, smv, wip, over_time, incentive, idle_time, idle_men, no_of_style_change, no_of_workers and actual_productivity. These were manually collected and validated by industry experts. In this project, the target variable is 'actual_productivity,' which is a continuous variable measured between 0 and 1, and indicates the actual percentage of productivity generated by garment workers.
VariableDescriptiondateThe date is in the format of month-day-year (MM-DD-YYYY).quarterThe month was split into four equal parts.departmentThe department that is linked to the particular instance.dayDay of the week.teamThe team number with the instance.targeted_productivityDaily productivity targets set by authority for each team.smvStandard Minute Value, it is the allocated time for a task.wipWork in progress. Includes the number of unfinished items for products.over_timeThe number of extra minutes worked by each team beyond their regular hours.incentiveThe monetary reward (in BDT) that encourages specific behaviour.idle_timeThe duration during which production was halted due to various factors.idle_menThe count of employees who were idle because of the production interruption.no_of_style_changeThe quantity of modifications made to the style of a specific product.no_of_workersNumber of employees in each team.actual_productivityActual percentage of productivity that was delivered by employees. Ranged from 0-1.Table 2.1: Variables' description
3. Business Understanding
3.1. Analysis Goal
　　　As this dataset is focused on the garment industry, which is highly labour-intensive; hence, the goal of this analysis is to implement an employee productivity prediction model using machine learning that accurately predicts the productivity level of employees considering various attributes. And, to evaluate which machine learning models work better in predicting the productivity of employees. Furthermore, we aim to identify the main features that contribute to productivity of employees in the garment industry. With these approaches, we can eventually increase the productivity level of employees, assisting businesses to meet the production targets within the designated timeline, reducing significant losses.
3.2. Analysis Data	
　　　The dataset used in this project is sourced from UCI Machine Learning Repository in 2020. In addition, the target variable of this dataset, actual_productivity is derived from attributes related to garment manufacturing and employee productivity.
4. Methodology
4.1. SEMMA Description
　　　The data analysis for this project utilises the SEMMA methodology, which stands for Sample, Explore, Modify, Model, and Assess. The SAS Enterprise Miner tool is employed to conduct SEMMA, which involves five major steps essential for a successful data mining project.
　　　In this assignment, we will primarily focus on the first two steps of SEMMA, Sample and Explore, which will be thoroughly discussed in the result section.
4.2. SEMMA Process
　　　Process of SEMMA is shown below:

5. Result
5.1. Sample
     The dataset used was saved as a comma-separated values (CSV) file. For data exploration, the CSV file was imported and saved as a SAS file. The steps for creating a new project, diagram, library, data source, and converting a CSV file into a SAS file are included in Appendix I. This dataset contained key characteristics of the garment manufacturing process and employee productivity that were personally collected and verified by specialists in the field, as shown in Table 2.1.
5.1.1. Metadata
     There are basic and advanced settings available in SAS Enterprise Miner to specify the data types of the variables. Figure 5.1 showed the metadata for the columns, and Figure 5.2 displayed the data types in their most basic configurations. Figure 5.3 showed the metadata for the columns, and Figure 5.4 displayed the data types in their default advanced settings. 

Figure 5.1: Column Metadata (Basic Setting)

Figure 5.2: Data Type Summary (Basic Setting)
     Figures 5.1 and 5.2 displayed that the data are categorised into interval and nominal as Input, ID, Target and Time ID with the default basic system settings, respectively. The measurement level was automatically determined by the system based on the potential values of the variables. Character data was classed as nominal type by default, whereas numeric data was classified as interval type. Some of the data roles and the data levels were still incorrect, therefore the basic settings were not suitable for implementation.

Figure 5.3: Column Metadata (Advanced Setting)

Figure 5.4: Data Type Summary (Advanced Setting)
     The data levels were categorised into binary, nominal, and interval after selecting the advanced setting, as shown in Figures 5.3 and 5.4. The system automatically recognised the variables' roles as Input, ID, Time ID and Target. There were still some errors where the data level did not correspond to the actual data level for the dataset and the target was identified wrongly. Additionally, the roles of idle_men and idle_time should be Input instead of ID. Therefore, manual amendment was required to amend the data roles and data levels before moving on to the Explore stage.
5.1.2. Reclassification of the Role and Level of the Variables
     To enable the production of appropriate charts for these variables, reclassification was done. Figure 5.5 depicted the data type summary after manual amendment and Figure 5.6 represented the data type summary which was finalised.

Figure 5.5: Reclassification of Role and Measurement Level of Variables

Figure 5.6: Data Type Summary (After Reclassification)
     All the variables were changed to Input since all the data had to be in before any analysis was done to support the deletion of the variables, as shown in Figure 5.5. actual_productivity, the target variable was manually amended. The date was converted into a nominal variable. no_of_style_change and targeted_productivity which were considered to be interval variables were also converted, and the remaining variables were left unchanged. 
     The data types were categorised into binary, nominal, and interval after selecting the advanced setting, as shown in Figures 5.6.
5.2. Explore
     In order to gain insight and ideas, this stage involves exploring the data by looking for hidden trends and abnormalities.
5.2.1. Summary Statistics
     A summary statistic was produced following accessing and analysing the dataset. Summary statistics are generated to provide a high-level overview of the data being analysed. The summary statistics for class variables are shown in Figure 5.7, while the summary statistics for interval variables are shown in Figure 5.8.

5.7: Class Variable Summary Statistics
     Figure 5.7 showed the number of levels and the mode of each class variable. Additionally, there were no missing values among the interval variables, as can be shown in Figure 5.8.

5.8: Interval Variable Summary Statistics
     Figure 5.8 provided a table of summary statistics for the interval variables. It was noted that wip had a total of 506 missing values in Figure 5.8. There was also a wide range of values spotted, as shown in Figure 5.8. Since these features are required to build the model, the wide range of values for each column suggests the need for scaling or normalisation. Besides, it was observed that 50% of the values for idle_men, idle_time, incentive, and no_of_style_change are zero, which might not provide useful insight.
5.2.2. Univariate Analysis
     Univariate analysis is a statistical technique used to examine and characterise a single variable. It entails analysing data and interpreting the findings solely on a single variable without taking into account how that variable interacts with other variables. 
     Pie charts and bar charts are used for categorical variables as these graphs are helpful in dividing categorical data distribution into numerical proportions. Pie charts are great for clearly displaying the proportions of categorical variables, especially when there are few categories. In the meanwhile, a bar chart is a more efficient approach to present information precisely and simply when there are a large number of categories.
     There were 5 categorical variables in the dataset which include date, day, department, quarter and team. The department and quarter, with few categories, were plotted using a pie chart whereas the date, day and team, with a large number of categories, were plotted using a bar chart, as shown in Figure 5.9 to Figure 5.13 respectively.

Figure 5.9: Pie Chart of department
     Based on Figure 5.9, it could be observed that there was probably a typo for the value of "sweing" which supposed to be "sewing". In contrast to the Finishing department, the Sewing department had a higher number of records, as can be shown in Figure 5.9.

Figure 5.10: Pie Chart of quarter
     Figure 5.10 showed that Quarter 1 had the most records, followed by Quarter 2, Quarter 4, Quarter 3, and Quarter 5.

Figure 5.11: Bar Chart of date
     According to Figure 5.11, 03/11/2015 and 01/31/2015 had the highest records of 24 each whereas 01/20/2015 had the lowest record of 15. 

Figure 5.12: Bar Chart of day
     Every day of the week had roughly the same number of records, with Saturday having the fewest and Wednesday having the most, as shown in Figure 5.12.

Figure 5.13: Bar Chart of team
     Figure 5.13 showed the two teams with the most records, 8, and 2, each having 109, are followed by team 1, which has 105. Team 11 had the fewest records of 88.
     Besides, there were 10 numerical variables which include idle_men, idle_time, incentive, no_of_style_change, no_of_workers, over_time, smv, targeted_productivity, wip and actual_productivity. In addition, idle_men, no_of_style_change and no_of_workers were discrete variables and the rest were continuous variables. Bar chart was used to plot the discrete variables, as shown in Figure 5.14 to Figure 5.16, while the histogram and box plot were used to plot the continuous variables, as shown in Figure 5.17 to Figure 5.18.

Figure 5.14: Bar Chart of idle_men

Figure 5.15: Bar Chart of no_of_style_change
     According to Figure 5.14 and Figure 5.15, it could be noted that both the idle_men and no_of_style_change only contained non-zero values in a tiny number of records which might not provide meaningful information and might lead to biased or incorrect results in the analysis. 

Figure 5.16: Bar Chart of no_of_workers
     There was a wide range in the number of workers where 8 workers had the most records (262), followed by 58 and 57 workers (135 and 134, respectively). There were numerous distinct worker counts, however they each had only a few records. Most importantly, it could be observed that there were float numbers such as 29.5, 33.5, 51.5 and so on in no_of_workers. Since it is impossible to have a fractional number of workers, the data type appears to be inconsistent and should be integers. 

Figure 5.17: Histogram of Continuous Variables
     According to Figure 5.17, it could be observed that most of the variables were heavily left-skewed and right-skewed. Multiple lower outliers in actual_productivity may have caused a slight left skew. Additionally, a missing bin with high frequency appeared in wip due to the high number of missing values. Due to the vast majority of the time, there is 0 idle time, 0 idle workers, and 0 incentive offered, resulting in a significant spike of 0 values in wip, idle_time and incentive. Most of the time, targeted productivity was set at 0.8. A task's allocated time was often between 18.40 minutes and 23.56 minutes, with the majority being 3.07 minutes or less. The overtime amount was usually between 5184 minutes and 7776 minutes, however, there is no overtime most of the time.

Figure 5.18: Boxplots of Numerical Variables
	As observed from the boxplots shown in Figure 5.18, there were no outliers in no_of_workers and smv. However, outliers were detected in idle_men, idle_time, actual_productivity, targeted_productivity, incentive, over_time, wip and no_of_style_change. The outliers in idle_men, idle_time and no_of_style_change probably due to the high frequency of 0 values. It was necessary to further investigate the nature of the outliers of the remaining variables to determine whether they were brought on by a mistake, omission, or natural behaviour.
5.2.3. Bivariate Analysis - Variable Association
Bivariate analysis is carried out to determine the relationship between two variables. Scatter plot is used for both numerical variables, bar chart is for two categorical variables and box plots are suitable for one categorical and one numerical variable. The findings are shown in Figure 5.19 to Figure 5.24.

Figure 5.19: Actual Productivity versus Target Productivity
The figure above showed the higher the target productivity, the higher the median of the actual productivity. However, we could observe a high frequency of employees unable to achieve their target starting from 0.7 target productivity, this may due to the productivity higher than 0.7 is hard to achieve.

Figure 5.20: Number of WIP in different Date
The number of WIP is approximately the same except for 02/02/2015 which has a sudden spike and goes back to normal for the next day. 

Figure 5.21: Number of Employee in Different Team and Department
From the bar chart above, we can observe sewing team 12 has the least number of works and team 4 is the highest. For finishing teams, team 4 also has the highest number of workers and team 6 has the least number of workers. 

Figure 5.22: Actual Productivity in Different Department
From the figure above we can observe the average and median of the finishing department is higher than the sewing department. This shows the productivity of the finishing department is higher than the sewing department.

Figure 5.23: Actual Productivity in Team
From the box plot above, we can observe that team 1 has the highest median of productivity and team 7 has the lowest median of productivity. 

Figure 5.24: The Actual Productivity in different OT hour
The figure showed that no pattern is found between the actual productivity with different OT hours.

Figure 5.25: The Actual Productivity in different incentive
The scatter plot showed the higher the actual productivity, the higher the incentive.




5.2.4. Multivariate Analysis
Multivariate Analysis is carried out to analyse the relation of multiple variables. The finding is result in Figure 5.26

Figure 5.26: Correlation Matrix
The purpose of carrying out correlation matrix besides analysing the relation of the variables, it also helps in detecting multicollinearity. Variable with correlation with other variables more than 0.9 is considered multicollinear and should be excluded from the dataset. Based on the correlation matrix, the multicollinearity does not exist in this dataset as all variables do not have correlation greater than 0.9. Therefore, no variable is removed due to multicollinearity. 
5.2.5. Interesting Visualization
The productivity of the employees in the garment manufacturing companies is important to fulfill the demand of the garment products as it is a highly labour-intensive industry. Hence, the productivity of each team and department is displayed in figure 5.27.

Figure 5.27 Actual Productivity in Different Department and Team
From the figure above, we can observe that the finishing department has higher productivity than the sewing department and team 1 performed the best in both finishing and sewing department. 





6. Conclusion
　　　The original dataset consisted of 15 attributes of varied data types: 1 binary variable, 4 nominal variables and 10 interval variables. After manually revising the metadata, the output is shown in the Table 6.1:
Role Variable typeCountInput  Binary  1Input Interval9Input Nominal4Target   Interval1Figure 6.1 Revised data
　　　During data exploration, we found outliers data. These variables are as follows:
VariableError typeidle_men Outliers idle_timeOutliers incentiveOutliersno_of_style_changeOutliersactual_productivityOutlierstargeted_productivityOutliersover_timeOutlierswip OutliersFigure 6.2 Data error type
We used the SAS Enterprise Miner variable selection algorithm to examine relevant variables. According to this algorithm, we find that some variables should be kept and some should be deleted. The specific reserved variables are as follows

NOVariable1date2quarter3department4day5team6targeted_productivity7smv8wip9over_time10no_of_workers11actual_productivityFigure 6.3 Concrete Reserved Variables
Next we will fix this data to make it suitable for model training in our next project.


?Appendices
　　　Appendix I: Sample
　　　Create a SAS Enterprise Miner Project.
¢ Select New Project
　　　
¢ Specify the project name
　　　
¢ Specify the project location and click Next
　　　


¢ Click Finish to create new project
　　　
　　　Create a SAS Enterprise Miner Diagram
¢ Select File > New > Diagram... from the main menu
¢ Specify the Diagram name and click OK
　　　
　　　
       

　　　Import data and save as SAS file
¢ Select Sample tab and drag File Import into the workspace window
　　　
¢ Click ... in under Train in the properties panel
　　　
¢ Browse the dataset to be imported
　　　
¢ Right-click the File Import node and select Run
　　　
¢ Select Utility tab and drag Save Data into the workspace window
　　　
¢ Connect File Import to Save Data, then right-click the Save Data node and select Run
　　　
　　　Create a library for the imported data file / SAS file
¢ Select File -> New -> Library... from the main menu
　　　
¢ Select Create New Library and click Next
　　　
¢ Specify the library name and insert the path information
　　　
¢ Click Finish
　　　
　　　Create data source and extract the imported SAS file
¢ Select File -> New -> Data Source... from the main menu
　　　
¢ Click Next
　　　
¢ Double-click the Dataset library, select the Em_save_train SAS Table and click OK
　　　
¢ Click Next
　　　
¢ Click Next
　　　
　　　
　　　
¢ Select the Advanced option and click Next
　　　
¢ Ensure the Role and Level columns are correct and click Next
　　　
¢ Click Next
　　　
¢ Click Finish
　　　
　　　Appendix II: Explore
　　　Generate Summary Statistics
¢ Drag the EM_SAVE_TRAIN data source to the workspace window
　　　
¢ Select Explore tab and drag StatExplore into the workspace window
	
¢ Connect EM_SAVE_TRAIN data to StatExplore, right-click the StatExplore node and select Run
	
¢ Click Results... to explore Summary Statistics
　　　
　　　Create Graph
¢ Select Explore tab and drag Graph Explore into the workspace window
	
¢ Connect EM_SAVE_TRAIN data to Graph Explore, right-click the Graph Explore node and select Run
	
¢ Click Results... 
	
¢ Select View tab, then select Plot... and choose the desired graph
	
¢ Specify the variable to visualise and click Finish.
	
	

　　　Add Missing Bin to Histogram
¢ Right click on the created histogram and select Graph Properties
	
¢ Tick "Show Missing Bin" and select OK
	
　　　Perform Variable Correlation
¢ Select Add Node > Explore > StatExplores
	
¢ Select View > Summary Statistics > Interval Variables
	
¢ Select Add Noed > Explore > Variable Clustering
　　　
¢ Select Model > Variable Correlation
　　　
       

　　　Interesting Visualisations
¢ Select Add Nodes > Explores > Graph Explore
　　　
¢ Select View > Plot
　　　
¢ Select Scatter
　　　
https://archive.ics.uci.edu/ml/datasets/in-vehicle+coupon+recommendation
7. Introduction
　　　As a popular marketing strategy, coupons have been widely adopted by businesses in various industries. According to the latest statistics, the market size of the global electronic coupon industry has exceeded 150 billion US dollars, and will grow rapidly at an annual rate of more than 20% in the next few years(Milena, 2021). However, the effectiveness of coupons largely depends on whether customers actually accept and use them. Therefore, it is very important for enterprises to accurately predict whether customers will accept a certain coupon, which can help enterprises formulate more targeted marketing plans, improve resource utilisation, and increase sales revenue(Kumar & Rajan, 2011).
　　　In this study, a dataset of real coupons from an e-commerce company in the United States is utilised. The dataset contains 12,684 records describing various driving scenarios, including the user's destination, current time, weather, passengers, coupon attributes, and other information about the user and the environment. The goal of this study is to develop a machine learning model that can accurately predict whether a customer will accept a particular coupon. By building this model, this study aims to provide a coupon recommendation system for enterprises, recommending coupons that are most likely to be accepted by users, so as to achieve higher resource utilisation efficiency and business value.
　　　To achieve this goal, the SEMMA (sample, explore, modify, model, evaluate, deploy) methodology is adopted to guide the research. In the sampling step, the metadata of the dataset is checked and the dirty data is cleaned. In the exploration step, this study delved into the dataset through univariate, bivariate, and multivariate analysis to identify the key factors that influence whether a customer accepts a coupon or not.
　　　In summary, the significance of this study lies in: first, through the analysis of the coupon data set, identify the key factors that affect users' acceptance of coupons, and provide theoretical support for building an accurate machine learning model and recommendation system; second, develop a coupon The coupon recommendation system provides technical support for companies to formulate coupon marketing strategies; thirdly, it provides examples for future customer insight and personalised recommendation research.
　　　In conclusion, this study aims to make a practical contribution to the precise and intelligent transformation of corporate coupon marketing through a data-driven approach.

8. Our Dataset
2.1 Dataset Description
The dataset was collected through a survey on Amazon Mechanical Turk and has been available on the UCI Machine Learning Repository since 2017.
　　　There are a total of 12684 instances and 26 attributes. It includes user-context attributes such as gender, age, marital status, income, education level, as well as demographic attributes like weather and temperature. In addition, there are general attributes such as the type of coupon and time before the coupon expires. The target variable is a binary label that indicates whether a user accepted a coupon recommendation or not. Table 2.1 showed the variables' description.
Table 2.1: Variables' description
VariableDescriptionDestinationDestination of the userPassangerThe user in the carWeatherThe state of weather when the user is drivingTemperatureThe degree of temperature when the user is drivingTimeThe time when user is drivingCouponThe type of coupon offeredExpirationThe time when the coupon is going to expire, 1 day or 2 hoursGenderUser's genderAgeUser's ageMarital statusUser's marital statusHas_ChildrenUser has children or no childrenEducationUser's educational levelOccupationUser's field of workIncomeUser's incomeCarDescription of vehicle which driven by userBarFrequency of user going to bar every monthCoffeeHouseFrequency of user going to coffee house every monthCarryAwayFrequency of user getting takeaway food every monthtoCoupon_GEQ15minDriving distance to the restaurant/bar for using the coupon is greater than 15 minutestoCoupon_GEQ25minDriving distance to the restaurant/bar for using the coupon is greater than 25 minutesDirection_sameWhether the restaurant/bar is in the same direction as the user's current destinationDirection_oppWhether the restaurant/bar is in the opposite direction as the user's current destinationYWhether the coupon is accepted or not
9. Business Understanding
3.1  Analysis Goal
　　　The goal of this analysis is to implement a coupon recommendation system using machine learning models that accurately predicts whether the user will accept a discount coupon or not. And, to evaluate which machine learning models work better in predicting whether the user will accept the coupon. Furthermore, this study aims to identify the main features that contribute to the acceptance of coupons in users. With these approaches, this study can eventually increase the coupon acceptance rate, assisting businesses to drive higher sales and revenue.
 3.2 Analysis Data
　　　The dataset used in this project is sourced from a survey on Amazon Mechanical Turk and it has been available publicly on the UCI Machine Learning Repository since 2017. In addition, the target variable of this dataset is derived from the user who opted to accept the coupon or not based on context, demographic and general variables.
10. Methodology
4.1  SEMMA Description
The data analysis for this project utilises the SEMMA methodology, which stands for Sample, Explore, Modify, Model, and Assess. The SAS Enterprise Miner tool is employed to conduct SEMMA, which involves five major steps essential for a successful data mining project.
　　　The first two steps of SEMMA, Sample and Explore will be primarily focused in this study, which will be thoroughly discussed in the result section.
4.2  SEMMA Process
Process of SEMMA is shown below:
                                                               
11. Result
11.1. Sample
     The dataset used was saved as a comma-separated values (CSV) file. For data exploration, the CSV file was imported and saved as a SAS file. The steps for creating a new project, diagram, library, data source, and converting a CSV file into a SAS file are included in Appendix I. This dataset contains key characteristics of the garment manufacturing process and employee productivity that were personally collected and verified by specialists in the field, as shown in Table 2.1.
11.1.1. Metadata
     There are basic and advanced settings available in SAS Enterprise Miner to specify the data types of the variables. Figure 5.1 displayed the metadata for the columns, and Figure 5.2 displayed the data types in their most basic configurations. Figure 5.3 displayed the metadata for the columns, and Figure 5.4 displayed the data types in their default advanced settings. 

Figure 5.1: Column Metadata (Basic Setting)

Figure 5.2: Data Type Summary (Basic Setting)
　　　Figures 5.1 and 5.2 displayed that the data types are categorised into interval and nominal as inputs with the default basic system settings, respectively. The measurement level was automatically determined by the system based on the potential values of the variables. Character data was classed as nominal type by default, whereas numeric data was classified as interval type. The majority of the data types were still incorrect, therefore the basic settings were not suitable for implementation.

Figure 5.3: Column Metadata (Advanced Setting)

Figure 5.4: Data Type Summary (Advanced Setting)
　　　The data types were categorised into binary, nominal, and unary after selecting the advanced setting, as shown in Figures 5.3 and 5.4. The system automatically recognised the variables' roles as input and rejected variables. 
　　　There were still some errors which the data types did not correspond to the actual data type for the dataset. Therefore, manual amendment was required to amend the data types before moving on to the Explore stage.
11.1.2. Reclassification of the Role and Level of the Variables
     To enable the production of appropriate charts for these variables, reclassification was done. Figure 5.5 depicted the data type summary after manual amendment and Figure 5.6 represented the data type summary which was finalised.

Figure 5.5: Comparison between role and measurement level between advance settings and manual amendment

Figure 5.6: Data Type Summary (After Reclassification)
     The rejected variables were changed to input variables since all the data had to be in before any analysis was done to support the deletion of the variables, as shown in Figure 5.5. Y, the target variable was manually amended. The unary variable toCoupon_GEQ5min was converted into a binary variable. Nominal and binary variables which were considered to be ordinal variables were also converted, and the remaining variables were left unchanged.
     The data types were categorised into binary, nominal, and ordinal after selecting the advanced setting, as shown in Figures 5.6.
11.2. Explore
     In order to gain insight and ideas, this stage involves exploring the data by looking for unexpected trends and abnormalities.
11.2.1. Summary Statistics
     A summary statistic was produced following accessing and assaying the dataset. Summary statistics are designed to give a high-level overview of the data being analysed. Since all the variables are class variables, the summary statistics was shown in Figure 5.7.

5.7: Class Variable Summary Statistics
     Figure 5.7 showed the number of levels and the mode of each class variable. Additionally, it was noted that a number of class variables, such as Bar, CarryAway, CoffeeHouse, Restaurant20To50, RestaurantLessThan20, and car, had missing values. As for the car variable, its missing values accounted for 12576 of 12684, that is to say, the missing values of car accounted for 99.15% in total.

Figure 5.8: Data Overview ( First 40 Records )
11.2.2. Univariate Analysis
     Univariate analysis is a statistical technique used to examine and characterise a single variable. It entails analysing data and interpreting the findings solely on a single variable without taking into account how that variable interacts with other variables. Since all the variables are categorical, the bar chart is the best graphical representation for these categorical variables. Rectangular bars are used in bar charts to display data, and the length of the bar corresponds to the frequency or proportion of the category. This kind of graph can be used to compare the relative sizes of various categories and show the frequency or proportion of each category in a variable. Figure 5. to Figure 5. displayed the bar charts of all the 26 variables. 

Figure 5.: Bar Chart of age

Figure 5.: Bar Chart of Bar

Figure 5.: Bar Chart of car

Figure 5.: Bar Chart of CarryAway

Figure 5.: Bar Chart of CoffeeHouse

Figure 5.: Bar Chart of coupon

Figure 5.: Bar Chart of destination

Figure 5.: Bar Chart of direction_opp

Figure 5.: Bar Chart of direction_same

Figure 5.: Bar Chart of education

Figure 5.: Bar Chart of expiration

Figure 5.: Bar Chart of gender

Figure 5.: Bar Chart of has_children

Figure 5.: Bar Chart of income

Figure 5.: Bar Chart of maritalStatus

Figure 5.: Bar Chart of occupation

Figure 5.: Bar Chart of passanger

Figure 5.: Bar Chart of Restaurant20To50

Figure 5.: Bar Chart of RestaurantLessThan20
Figure 5.: Bar Chart of temperature

Figure 5.: Bar Chart of time

Figure 5.: Bar Chart of toCoupon_GEQ5min

Figure 5.: Bar Chart of toCoupon_GEQ15min

Figure 5.: Bar Chart of toCoupon_GEQ25min

Figure 5.: Bar Chart of weather

Figure 5.: Bar Chart of Y
     Based on the univariate analysis, it is observed that there is a significant number of null values in Bar, car, CarryAway, CoffeeHouse, Restaurant20To50 and RestaurantLessThan20 which require the mode imputation in the Modify stage. However, the extremely large number of missing values in the car variable indicates that the variable needs to be dropped during the Modify stage. Besides, the toCoupon_GEQ5min variable has only 1 unique value which means that it is not useful for the modelling, as shown in Figure 5.. In addition, according to Figure 5. and Figure 5., the value of direction_opp is complementary to the value of direction_same. As a result, only 1 of them is needed. Therefore, the 3 variables, car, toCoupon_GEQ5min and direction_opp are needed to be dropped during the Modify stage.
11.2.3. Bivariate Analysis - Variable Association
Bivariate Analysis is carried out to test the relationship between the features and the target variables. Before looking into each category, SAS Enterprise Miner variable selection is used to check the relevant variables. (Figure)

Figure: Feature Selection
The figure above is the result of the variable selection. The figure indicates that type of coupon, coupon expiration, Coffee House, passenger, occupation, time, restaurant20To50, Coupon_GEQ15min, bar and destination are worth keeping. The other variables are dropped.
To confirm the findings,  the relationship of the variable with the target variable is visualised. 
Bivariate analysis analyses the relationship of two variables. Since all variables are category variables. Bar chart is applied to show the bivariate analysis.
Demographic of Driver:



　　　From the figure above, it can be observed that the frequency of drivers accepting the coupon is higher than rejecting the coupon regardless of any demographic in age, gender, maritalStatus, has_children, income, passenger and education. However, as passengers, it can be seen that drivers with friends have a significantly higher percentage of drivers accepting the coupon. For occupations of the drivers, most occupations have a higher frequency of accepting the coupon except for Art Design Entertainment Sports and Media. Students show a significantly higher percentage of drivers accepting the coupon. 
　　　Therefore, it can be concluded that from the demographic of driver perspective, the variable does not show significant differences for the acceptance of the coupon except for passenger and occupation in which driver who is student and driving with friends will have higher tendency accepting the coupon.

Eating Habits of the Driver:

　　　Figure above showed the eating habits of the driver influence the acceptance of the coupon by the driver. Bar and Coffee House indicates the frequency of the driver going to a bar or coffee house in a month. The figures show that with 1 to 3 frequencies to bar or coffee house have a higher percentage of accepting the coupon. For the carry away frequency, the figure shows approximately the same distribution of acceptance of drivers for different frequency Restaurant less than 20 means the frequency of going to a restaurant with an average expense per person of less than $20 every month. The figure shows the approximate same distribution of acceptance of the driver for different frequencies. Another figure of restaurant 20 to 50 means the frequency of going to a restaurant with an average expense per person of between $20 and $50 every month. This figure shows that frequency less than one has the higher chance of accepting the coupon.

　　　Conclusion, drivers that went to bar and coffee house frequently and went to restaurant of an average expense per person between $20 and $50 less frequently will accept the coupon.

Type of coupon and the weather when the coupon is distributed:

　　　The figure shows that coupons for restaurant of an average of expense per person less than 20 will have a higher tendency of accepting. Coupons that expire in one day also have a higher tendency of accepting that coupon that expires in 2 hours. Weather and temperature does not show any effect on these two factors affecting the acceptance of the coupon. Finally, the coupon has a higher tendency of being accepted when it is distributed at 2pm.

Destination of the driver and the restaurant location


　　　The figure shows how the destination of the driver will affect the acceptance of the coupon. First figure showed that more drivers to no urgent place will accept the coupon and no accepting the coupon when going back home.Second and third figure showed most drivers driving to the same direction to the restaurant/ bar, however, both same or different directions also showed that higher number of drivers will accept the coupon. Therefore, this concludes that the direction of the driver does not affect the acceptance of the coupon. The fourth, fifth and sixth figure showed whether the restaurant/bar could have arrived greater than 15 minutes, 25 minutes and 5 minutes respectively. The figure showed that if the promoted restaurant could arrive in 15 minutes, more drivers would accept the coupon. 
11.2.4. Multivariate Analysis

Correlation matrix table is plotted to analyse the correlation of two or more elements of variables that are correlated to measure the strength of relationship of the two factors. The strength of the relationship is measured from blue to red, with blue indicating weak correlation and red box indicating strong correlation. Factors that have correlation of 0.9 and above should be removed to avoid multicollinearity. Multicollinearity should be avoided to reduce harmful bias in the model. According to the figure above, there are no correlation values greater than 0.90, therefore, there are no variables removed from the model.
11.2.5. Interesting Visualization
(1) What does the distribution look like for bar and restaurant20and50 by the response of the driver?


(2) What does the distribution look like for coffeehouse and restaurant20and50 by the response of the driver?



(3) What does the distribution look like for expiration and Coupon_GEQ15min, by the response of the driver?






12. Conclusion
In the "Vehicle Coupon Recommendation Dataset", the initial metadata information has 26 variables, including 8 binary input variables, 15 nominal input variables, 2 rejected nominal variables and 1 rejected unary variable. After we modified the data, it became like this:
Role Variable typeCountInput   Binary   7Input Nomina 8Input  Ordina  10Target   Binary  1Figure 6.1 Revised data
This indicates that some variables have been reclassified or removed during the Modify stage. Rejected nominal and univariate variables were removed from the dataset. In addition, some nominal variables have been transformed into ordinal variables for analysis and modeling. These changes make the dataset more suitable for subsequent analysis and model building.

During data exploration, we found incomplete data, noisy data, inconsistent data, and redundant data. These variables are as follows:
VariableError typecarIncompletetoCoupon_GEQ5mininconsistencydirection_oppredundancyFigure 6.2  Error type
We used the SAS Enterprise Miner variable selection algorithm to examine relevant variables. According to this algorithm, we find that some variables should be kept and some should be deleted. The specific reserved variables are as follows

NOVariable1type of coupon2 coupon expiration3Coffee House4passenger5 occupation6time7restaurant20To508Coupon_GEQ15min9 bar 10 destination Figure 6.3 Concrete Reserved Variables
Next we will fix this data to make it suitable for model training in our next project.
Reference
Milena. (2021, May 30). Coupon Statistics - 2023 Update | Balancing Everything. Balancing Everything. https://balancingeverything.com/coupon-statistics/
Kumar, V., & Rajan, B. (2011). Social coupons as a marketing strategy: A multifaceted perspective. ResearchGate; SAGE Publications. https://www.researchgate.net/publication/225182866_Social_coupons_as_a_marketing_strategy_A_multifaceted_perspective
